{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Python\n",
    "\n",
    "- Tools: scikit-learn (`sklearn`)\n",
    "    - Data Partitioning\n",
    "    - Feature selection\n",
    "    - Modeling: SVM\n",
    "    - Model Assessment\n",
    "    \n",
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS108/Lectures-Sp25/blob/main/13_ml.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more reading on scikit-learn (`sklearn`) and machine learning in Python: https://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: General Steps\n",
    "\n",
    "1. Data Partitioning\n",
    "2. Feature Selection\n",
    "3. Fit a model\n",
    "4. Model Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ds/plotting packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Support Vector Machines\n",
    "\n",
    "- simple & interpretable machine learning model\n",
    "- based in linear regression\n",
    "- classification task\n",
    "- supervized\n",
    "    - input: labeled training data\n",
    "    - model determines hyperplane that best discriminates between categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM: Tuning Parameters\n",
    "- **regularization** parameter `C`\n",
    "    - determines how the decision boundary is drawn\n",
    "    - values << 1 are hard margin, try to draw perfect boundary, not good for noise\n",
    "    - values >> 1 are soft margin, allow for imperfections like orange class in the blue class territory\n",
    "- **kernel** parameter `kernel`\n",
    "    - specifies how to model & transform data\n",
    "    - can be linear or non linear\n",
    "    - RBF is very general purpose non linear\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more reading on SVMs using `sklearn`: https://scikit-learn.org/stable/modules/svm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Assessment\n",
    "\n",
    "- for continuous prediction / regression tasks\n",
    "    - MSE, RMSE, MAE\n",
    "- for classification tasks\n",
    "    - Accuracy\n",
    "    - TP, TN, FP, FN\n",
    "    - Precision, Recall, AUC \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![confusion matrix](https://miro.medium.com/max/924/1*7EYylA6XlXSGBCF77j_rOA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sensitivity recell](https://miro.medium.com/max/878/1*Ub0nZTXYT8MxLzrz0P7jPA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** - What % were predicted correctly?  \n",
    "**Sensitivity (Recall)** - Of those that were positives, what % were predicted to be positive?  ; $\\frac {TP}{(TP + FN)}$  \n",
    "**Specificity** - Of those that were actually negatives, what % were predicted to be negative?  $\\frac {TN}{(TN + FP)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision (Positive Predictive Value, PPV)** = $\\frac {TP}{(TP + FP)}$\n",
    "\n",
    "- probability that predicted positive truly is positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A short EC quiz checkin\n",
    "\n",
    "\n",
    "Click this link:\n",
    "\n",
    "https://docs.google.com/forms/d/e/1FAIpQLScI2vt2rW7rjRVQXKEQLuC68RICLUXa-Vyo5TXcPYtGTW7aHw/viewform?usp=sharing&ouid=101992505427251219835\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "One thing we didn't talk about is how you need to engineer features for NON-text kinds of inputs\n",
    "\n",
    "In general this is a huge topic that I will try condense for you into two bullet points.\n",
    "\n",
    "- If your data has different kinds of continuous variables, and especially if some of those variables tend to be big numbers (distance to the moon in meters) and some tend to be small numbers (the earths mass as a fraction of the earth's mass), then you need to STANDARDIZE those two variables to have roughly the same magnitude. If you don't then the ML system may think that the large magnitude variable is more predictive than the small magnitude variable.  See this link https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler\n",
    "- If your data has categorical variables that do not have a natural order to them (red is not greater than blue), then you need to ONE-HOT encode those categorical variables.  Otherwise your ML system will think that red is greater than blue and that can make for some bad predictions.  See this link https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-categorical-features\n",
    "\n",
    "OK lets talk about how to do things with the non text input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# lets get some ðŸ§ data to work with\n",
    "df = sns.load_dataset('penguins').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='species');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... lets take a close look at body mass vs bill length.... that seems really promising!!\n",
    "\n",
    "Let's imagine what it would be like to use k-Nearest Neighbors on only those two inputs... it might be enough to make things predict well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we want to predict\n",
    "y = df['species']\n",
    "# the data we will predict from\n",
    "X = df[['bill_length_mm','body_mass_g']]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UGH!!! the vector difference $\\mathbf{x}_{new} -  \\mathbf{x}_{i}$ is going to be absolutely dominated by body mass... it's two orders of magnitude bigger than bill length!\n",
    "\n",
    "We may not want that!  From the scatterplot it looks like both variables are equally contibuting to understanding species.\n",
    "\n",
    "We need to normalize the data so each variable is a z-scored version of itself... that means both variables will now be on roughly the same order of magnitued (about -3 to 3).  There are in fact many diffrerent kinds of normalization that could work here!  Take a look at this list: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE \n",
    "# import and instantiate a standard scaler\n",
    "# assign the scaler to preproc\n",
    "\n",
    "preproc.fit(X)\n",
    "scaled_data = preproc.transform(X)\n",
    "X['bill_length_mm'] = scaled_data[:,0]\n",
    "X['body_mass_g'] = scaled_data[:,1]\n",
    "\n",
    "X\n",
    "\n",
    "# some things to note here... we could just use scaled_data instead of stuffing\n",
    "# the results back into the dataframe X... I'm just doing this so we can keep\n",
    "# the column labels intact... scaled_data is a numpy ndarray without those labels\n",
    "# sklearn will accept numpy arrays or pandas dataframes/series or any other iterable \n",
    "# as input, so use whatever you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we decided to learn above is strictly numerical.  And preprocessing the way we just described is only good for numerical data.\n",
    "\n",
    "What if we had been interested in categorical data? For categorical data inputs we usually wish to use a one-hot encoding.\n",
    "\n",
    "One-hotting a single categorical variable that has N possible categories makes a vector representation that is N long. Because one-hot assumes each category is mutually exclusive with every other category, only one of those N category holders is allowed to be non-zero.\n",
    "\n",
    "Why use one-hot encoding?  This eliminates unwanted \"fake relationships\" in categorical data that a different encoding might cause an ML system to learn.\n",
    "\n",
    "Imagine we have categories like (red, blue, green). There is no ordinal relationship here... red is not > blue, nor is blue > green.  But if we turned (red, blue, green) into a representation like (3, 2, 1) that is exactly what our ML system would learn. Even worse, it would be more likely to confuse blue and green than red and green... because blue-green is smaller than red-green.\n",
    "\n",
    "One hot eliminates this because the 3 long vectors representing each color [1, 0, 0], [0, 1,0], and [0, 0, 0] are orthogonal (unrelated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE \n",
    "# import and instantiate a one hot encoder\n",
    "# assign the encoder to catxform\n",
    "\n",
    "onehot = catxform.fit_transform(df[['island','sex']])\n",
    "print(df[['island','sex']])\n",
    "print(onehot.todense())\n",
    "\n",
    "# there are 3 different islands, so 3 columns get used to encode which island\n",
    "# only 1 of those 3 columns can be \"hot\" (non-zero) for each penguin\n",
    "# likewise there are 2 different sexes recorded here, so 2 columns get used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anyway we're not going to use categorical data right now, so lets stick to the numerical data we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "pclassifier.fit(X,y)\n",
    "predicted = pclassifier.predict(X)\n",
    "\n",
    "print(classification_report(y,predicted))\n",
    "\n",
    "cm = confusion_matrix(y,predicted)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result shows us that with just 2 variables we can get to 95% accuracy!\n",
    "\n",
    "BUT note this is TRAINING SET accuracy... something of limited use.\n",
    "\n",
    "Let's figure out how to make a training set/test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
    "\n",
    "print('training/test set sizes')\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "pclassifier.fit(X_train, y_train)\n",
    "predicted_train = pclassifier.predict(X_train)\n",
    "predicted_test = pclassifier.predict(X_test)\n",
    "\n",
    "print('training set performance')\n",
    "print(classification_report(y_train,predicted_train))\n",
    "print()\n",
    "print('test set performance')\n",
    "print(classification_report(y_test,predicted_test))\n",
    "print()\n",
    "print('test set confusion matrix')\n",
    "cm = confusion_matrix(y_test,predicted_test)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well what if instead of train/test, we do cross validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "print('lets do 5 fold cross validation...')\n",
    "X_xval, X_test, y_xval, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
    "\n",
    "print('xval/test set sizes')\n",
    "print(X_xval.shape, X_test.shape, y_xval.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "pclassifier = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "predicted_xval = cross_val_predict(pclassifier, X_xval, y_xval, cv=5)\n",
    "predicted_test = pclassifier.fit(X_xval,y_xval).predict(X_test)\n",
    "\n",
    "print('xval performance')\n",
    "print(classification_report(y_xval,predicted_xval))\n",
    "print()\n",
    "print('test set performance')\n",
    "print(classification_report(y_test,predicted_test))\n",
    "print()\n",
    "print('test set confusion matrix')\n",
    "cm = confusion_matrix(y_test,predicted_test)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow... but cross-validation is really useful for looking at the BEST set of hyperparameters possible... is there a way to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "print('lets do grid search using 5 fold cross validation... and this time lets use all the measurements not just the best two')\n",
    "X = df[['bill_length_mm','bill_depth_mm','flipper_length_mm', 'body_mass_g']]\n",
    "y = df['species']\n",
    "\n",
    "preproc = StandardScaler()\n",
    "\n",
    "preproc.fit(X)\n",
    "scaled_data = preproc.transform(X)\n",
    "X['bill_length_mm'] = scaled_data[:,0]\n",
    "X['bill_depth_mm'] = scaled_data[:,1]\n",
    "X['flipper_length_mm'] = scaled_data[:,2]\n",
    "X['body_mass_g'] = scaled_data[:,3]\n",
    "\n",
    "X_xval, X_test, y_xval, y_test = train_test_split( X, y, test_size=0.3, random_state=777)\n",
    "\n",
    "print('xval/test set sizes')\n",
    "print(X_xval.shape, X_test.shape, y_xval.shape, y_test.shape)\n",
    "print()\n",
    "\n",
    "# note we don't need to put n_neightbors here now :)\n",
    "pclassifier = KNeighborsClassifier()\n",
    "params = { 'n_neighbors':[1,3,5,7,9,11,13,15]}\n",
    "\n",
    "searcher = GridSearchCV(pclassifier, param_grid=params, cv=5)\n",
    "\n",
    "# at this point we run a bunch of cross validations\n",
    "searcher.fit(X_xval, y_xval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fitted set of cvs, one for each param set in the grid\n",
    "searcher.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can match this up with mean_test_score above\n",
    "searcher.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searcher, when used as a classifier, uses the best_params_ by default\n",
    "predicted_test = searcher.fit(X_xval,y_xval).predict(X_test)\n",
    "\n",
    "\n",
    "print('test set performance')\n",
    "print(classification_report(y_test,predicted_test))\n",
    "print()\n",
    "print('test set confusion matrix')\n",
    "cm = confusion_matrix(y_test,predicted_test)\n",
    "disp = ConfusionMatrixDisplay(cm)\n",
    "disp.plot()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines are another ML technique\n",
    "\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=0, cluster_std=0.60)\n",
    "\n",
    "xfit = np.linspace(-1, 3.5)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "\n",
    "for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.4)\n",
    "\n",
    "plt.xlim(-1, 3.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In support vector machines, the line that maximizes this margin is the one we will choose as the optimal model.\n",
    "Support vector machines are an example of such a *maximum margin* estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a support vector machine\n",
    "\n",
    "Let's see the result of an actual fit to this data: we will use Scikit-Learn's support vector classifier to train an SVM model on this data.\n",
    "For the time being, we will use a linear kernel and set the ``C`` parameter to a very large number (we'll discuss the meaning of these in more depth momentarily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # \"Support vector classifier\"\n",
    "model = SVC(kernel='linear', C=1E10)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better visualize what's happening here, let's create a quick convenience function that will plot SVM decision boundaries for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=3, facecolors='none', edgecolor='k');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "\n",
    "plot_svc_decision_function(model)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to this classifier's success is that for the fit, only the position of the support vectors matter; any points further from the margin which are on the correct side do not modify the fit!\n",
    "Technically, this is because these points do not contribute to the loss function used to fit the model, so their position and number do not matter so long as they do not cross the margin.\n",
    "\n",
    "We can see this, for example, if we plot the model learned from the first 60 points and first 120 points of this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.60)\n",
    "    X = X[:N]\n",
    "    y = y[:N]\n",
    "    model = SVC(kernel='linear', C=1E10)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    ax = ax or plt.gca()\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=ax)\n",
    "    ax.set_xlim(-1, 4)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plot_svc_decision_function(model, ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "for axi, N in zip(ax, [30, 60, 120]):\n",
    "    plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the left panel, we see the model and the support vectors for just 10 training points, in the middle we see  60 training points and on the right panel 120 points.  While going from 30 to 60 caused changes, going from 60 to 120 did not becuase the support vectors (the points on the margin line) had not changed.\n",
    "\n",
    "This insensitivity to the exact behavior of distant points is one of the strengths of the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond linear boundaries: Kernel SVM\n",
    "\n",
    "Where SVM becomes extremely powerful is when it is combined with *kernels*.\n",
    "We have seen a version of kernels before: Remember polynomial features from the regression lectures?  \n",
    "\n",
    "There we projected our data into higher-dimensional space defined by polynomials functions, and thereby were able to fit for nonlinear relationships with a linear regression.\n",
    "\n",
    "In SVM models, we can use a version of the same idea.\n",
    "To motivate the need for kernels, let's look at some data that is not linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1)\n",
    "\n",
    "clf = SVC(kernel='linear').fit(X, y)\n",
    "\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show();\n",
    "#plot_svc_decision_function(clf, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that no linear discrimination will *ever* be able to separate this data.\n",
    "\n",
    "One simple projection we could use would be to compute a *radial basis function* centered on the middle clump:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding phi as a 3rd dimension gives us this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap(sns.color_palette()[:2])\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(X[:, 0], X[:, 1], phi, c=y, s=7, cmap=cmap)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('$\\phi(\\mathbf{x})_3$');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with this additional dimension, the data becomes trivially linearly separable, by drawing a separating plane at, say, *phi*=0.7.\n",
    "\n",
    "Here we had to choose and carefully tune our projection: if we had not centered our radial basis function in the right location, we would not have seen such clean, linearly separable results.\n",
    "In general, the need to make such a choice is a problem: we would like to somehow automatically find the best basis functions to use.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "A potential problem with this strategyâ€”projecting $N$ points into $N$ dimensionsâ€”is that it might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitlyâ€”that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "In Scikit-Learn, we can apply kernelized SVM simply by changing our linear kernel to an RBF (radial basis function) kernel, using the ``kernel`` model hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', C=1)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75)\n",
    "plot_svc_decision_function(clf)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1],\n",
    "            s=300, lw=1, facecolors='none', edgecolor='k');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this kernelized support vector machine, we learn a suitable nonlinear decision boundary.\n",
    "This kernel transformation strategy is used often in machine learning to turn fast linear methods into fast nonlinear methods, especially for models in which the kernel trick can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the SVM: Softening Margins\n",
    "\n",
    "Our discussion thus far has centered around very clean datasets, in which a perfect decision boundary exists.\n",
    "But what if your data has some amount of overlap?\n",
    "For example, you may have data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75);\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle this case, the SVM implementation has a bit of a fudge-factor which \"softens\" the margin: that is, it allows some of the points to creep into the margin if that allows a better fit.\n",
    "The hardness of the margin is controlled by a tuning parameter, most often known as $C$.\n",
    "For very large $C$, the margin is hard, and points cannot lie in it.\n",
    "For smaller $C$, the margin is softer, and can grow to encompass some points.\n",
    "\n",
    "The plot shown below gives a visual picture of how a changing $C$ parameter affects the final fit, via the softening of the margin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=0.8)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    model = SVC(kernel='linear', C=C).fit(X, y)\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, s=75, ax=axi)\n",
    "    plot_svc_decision_function(model, axi)\n",
    "    axi.scatter(model.support_vectors_[:, 0],\n",
    "                model.support_vectors_[:, 1],\n",
    "                s=300, lw=1, facecolors='none', edgecolor='k');\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar model selection procedure.\n",
    "\n",
    "\n",
    "In fact in general we would want to do model selection on all the hyperparameters of the SVM... $C$, what kernel to use, and whatever hyperparameters are unique to each kernel like polynomial order in $\\phi$.\n",
    "\n",
    "In fact sklearn gives us a powerful tool to do exactly this. Which I will demonstrate below.\n",
    "\n",
    "If you're actually following us all the way to this point you are getting into the territory of COGS118A, rather than COGS 108.  So don't worry too much if you feel like you're struggling to understand... this is advanced stuff \n",
    "\n",
    "\n",
    "But the code below will provide you a template for doing complex model and feature selection using sklearn.  Something that may be useful to your projects :)\n",
    "\n",
    "Please be careful... this is a template, not something you can necessarily copy/paste for your own data.  You will need to understand how each element works, and how your data might need something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this will take around 2-3 minutes to run\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "\n",
    "# what we want to predict\n",
    "y = df['species']\n",
    "# the data we will predict from... note that we will include SEX\n",
    "# because penguin sex is actually pretty important in their size measurements\n",
    "X = df.drop(columns=['island','species'])\n",
    "\n",
    "# sex is a categorical variable... it needs to be one hot encoded!\n",
    "# we know from before that we need to z-score the numeric variables\n",
    "\n",
    "# here we setup a way to transform the categorical variables into one hot, \n",
    "# and to z-score the numeric variables.... completely automatically!!\n",
    "# remember how we had to do this with a bunch of different\n",
    "# steps manually the last time around??\n",
    "# if you'd like to understand how to do this more generally\n",
    "# search the sklearn docs for ColumnTransformer!\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', onehot, categorical_columns),\n",
    "    ('zscore', scaler, numerical_columns)])\n",
    "\n",
    "\n",
    "# Create a pipeline \n",
    "# a pipeline puts together two (or 3 or however many) steps into one sklearn\n",
    "# estimator. That means we can use pipe instead of svc in any xval or grid search\n",
    "# and get the preprocessing done for us in the same step\n",
    "pipe = Pipeline([('make_features', preprocessor),\n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# SVM have very different hyperparameters depending on which kernel\n",
    "# potential hyperparams are gamma, coef0, degree and kernel.\n",
    "# here <x,x'> and ||x-x'|| are similarities between two vectors x,x'\n",
    "# linear kernel:     <x,x'>  [no hyperparams]\n",
    "# poly kernel:       ( gamma * <x,x'> + coef0 )^degree \n",
    "# rbf kernel:        exp( - gamma * ||x-x'||^2 )\n",
    "# sigmoid kernel:    tanh( gamma * <x,x'> + coef0 )\n",
    "\n",
    "# so how can we search using only using hypers appropriate for each kernel?\n",
    "# like this...make a list of different search spaces you want to execute\n",
    "search_space = [{'classifier__kernel': ['linear'],\n",
    "                 'classifier__C': np.logspace(-3, 2, 11) #11 steps between 10^-3 to 10^2\n",
    "                },\n",
    "                {'classifier__kernel': ['poly'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__degree': range(2,14),\n",
    "                 # NOPE... this isnt often important 'classifier__coef0': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                {'classifier__kernel': ['rbf'],\n",
    "                 'classifier__gamma': np.logspace(-3, 2, 11),\n",
    "                 'classifier__C': np.logspace(-3, 2, 11)\n",
    "                },\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# Create a grid search object to find the best model\n",
    "best_model = GridSearchCV(pipe, search_space, cv=5, verbose=1)\n",
    "# play with different verbose=??\n",
    "\n",
    "# now lets split off some of our data for testing set... \n",
    "# the rest will be used for cross validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, random_state=101)\n",
    "\n",
    "\n",
    "# Fit grid search\n",
    "%time best_model.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find out how well we do on the test set...\n",
    "print(best_model.best_params_)\n",
    "yhat = best_model.predict(X_test)\n",
    "print(classification_report(y_test, yhat))\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, yhat);\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
